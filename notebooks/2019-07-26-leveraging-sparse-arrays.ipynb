{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Sparse Arrays for Large-ish (Feature) Data Sets\n",
    "\n",
    "When building a machine learning model, you'll often have your feature data stored in one or more database tables (e.g., with a couple of million rows and maybe a thousand columns) where each row represents, say, an individual user and each column represents some feature that you've engineered (i.e., number of logins to your site, number of daily purchases, number of ads clicked on, 52 consecutive columns storing a weekly moving average over a year, etc). Since storage is cheap and you can't afford to pay for a compute cluster, your instinct might be to download the full table onto your laptop as a quick and dirty CSV file or, possibly, in a more portable and compact parquet format. However, depending on your local computer hardware, trying to load that full data set into a Pandas DataFrame might consume all of your memory and cause your system to grind to a halt!\n",
    "\n",
    "Luckily, there is one approach that a lot of people often overlook but that might work wonderfully for you. The secret is to exploit the inherent sparsity that likely exists within your large data set! \n",
    "\n",
    "As an illustrative example, let's consider the following list of ones and zeros:\n",
    "\n",
    "`[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`\n",
    "\n",
    "To capture all of the information to represent this list, all that you really need is:\n",
    "\n",
    "1. The total length of the list\n",
    "2. The exact position of each `1` (the position of each `0` is implied)\n",
    "\n",
    "In other words, since the majority of the list is full of zeros, we can start by assuming that the entire list is full of zeros with the exception of the locations of the `1`s. In doing so, we end up only needing to record a few bits of information and thereby saving us a ton of memory to do other interesting things. The number of nonzero elements that exist in your matrix/array also defines the `density` of your matrix. That is, the more nonzero elements that you have (relative to the total number of elements) then the higher the density. A sparse matrix/array would have very low density (i.e., less than 25% dense) but, in the case of machine learning, you run the risk of not having enough information to learn from if your matrix/array is too sparse (i.e., less than 5% dense). This philisophical discussion is probably beyond the scope of this blog post, which is to explain how to take your data and to create a sparse matrix/array that can be used for building a machine learning model.\n",
    "\n",
    "In fact, SciPy already offers fantastic support for building your own [sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html). However, to future-proof ourselves from a soon-to-be deprecated 2D numpy matrix format, we'll be leveraging the [PyData Sparse](https://sparse.pydata.org/en/latest/index.html) package for all of our sparse nd-array needs. So, in places below where you see \"sparse matrix\", know that we really mean a \"2D array\" but, unlike a matrix, the array can be generalized to higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So What?\n",
    "\n",
    "If you already have your data in hand then, most of the time, converting that numpy array or pandas dataframe into a sparse matrix is pretty trivial. However, where things get really tricky is when your data is stored within one giant table/file that you can't load into memory all at once or it is stored across multiple medium sized tables that you'll then need to figure out how to stitch back together in Python. In the latter case, each chunk/table/file will contain the same number of rows (i.e., each row represents the same user) but each chunk/table/file will only contain a different subset of feature columns. \n",
    "\n",
    "So, the pseudocode for converting this data into a sparse matrix might look something like:\n",
    "\n",
    "```\n",
    "sparse_matrix_list = []\n",
    "db_cnxn = connect_to_db()\n",
    "for chunk in db_cnxn.get(chunks):\n",
    "    sparse_chunk = convert_chunk_to_sparse(chunk)\n",
    "    sparse_matrix_list.append(sparse_chunk)\n",
    "```\n",
    "\n",
    "At first glance, this seems pretty straightforward. You retrieve only one chunk at a time, process each chunk, convert that chunk into a sparse matrix, and append it to a growing list so that it can be combined into a single large sparse matrix. But once you dig into it, you'll discover all the ways that things can go wrong! Below, we'll show you one relatively clean approach that will help to keep everything in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparse\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Wide Data\n",
    "\n",
    "Let's pretend that the dataframe below represents our large data which is stored in some remote database or chunked up. Remember, since the data is \"big\", the database only allows you to fetch, say, five columns of data at a time and, realistically, you don't want to have to write any of this data to disk if at all possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>feature_a</th>\n",
       "      <th>feature_b</th>\n",
       "      <th>feature_c</th>\n",
       "      <th>feature_d</th>\n",
       "      <th>feature_e</th>\n",
       "      <th>feature_f</th>\n",
       "      <th>feature_g</th>\n",
       "      <th>feature_h</th>\n",
       "      <th>feature_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  feature_a  feature_b  feature_c  feature_d  feature_e  feature_f  \\\n",
       "0       x          1          2          3          6          0          0   \n",
       "1       y          2          3          0          0          0          4   \n",
       "2       z          3          0          1          0          1          2   \n",
       "3       w          0          0          0          0          3          0   \n",
       "\n",
       "   feature_g  feature_h  feature_i  \n",
       "0          7          0          0  \n",
       "1          8          0          5  \n",
       "2          0          0          9  \n",
       "3          1          0          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.DataFrame([['x', 1, 2, 3, 6, 0, 0, 7, 0, 0], \n",
    "                       ['y', 2, 3, 0, 0, 0, 4, 8, 0, 5], \n",
    "                       ['z', 3, 0, 1, 0, 1, 2, 0, 0, 9], \n",
    "                       ['w', 0, 0, 0, 0, 3, 0, 1, 0, 0]], \n",
    "                      columns=['user_id', \n",
    "                               'feature_a', \n",
    "                               'feature_b', \n",
    "                               'feature_c', \n",
    "                               'feature_d', \n",
    "                               'feature_e', \n",
    "                               'feature_f', \n",
    "                               'feature_g', \n",
    "                               'feature_h', \n",
    "                               'feature_i'])\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimic a Chunked Data Source\n",
    "\n",
    "Here, we use a Python generator to pretend like we are accessing our fictitious database one subset of columns at a time. But, in the real world, this will be replaced by your database connection of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_chunks(df):\n",
    "    for chunk in [['user_id', 'feature_a', 'feature_b', 'feature_c'], \n",
    "                  ['user_id', 'feature_d', 'feature_e', 'feature_f', 'feature_g'], \n",
    "                  ['user_id', 'feature_h', 'feature_i']]:\n",
    "        \n",
    "        yield df[chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could retrieve our first chunk directly from our databse but there are a couple of issues:\n",
    "\n",
    "1. The chunk itself is too big as it contains a lot of zeros\n",
    "2. SciPy expects the data to be a long format instead of a wide format\n",
    "\n",
    "So, we should try to get the database to do as much of the heavy lifting as possible such as to pivot the wide tables into a long format and then to remove all zeros and only return a long table with nonzero entries. Let's modify our `get_df_chunks` generator from above to reflect this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_chunks(df):\n",
    "    for chunk in [['user_id', 'feature_a', 'feature_b', 'feature_c'], \n",
    "                  ['user_id', 'feature_d', 'feature_e', 'feature_f', 'feature_g'], \n",
    "                  ['user_id', 'feature_h', 'feature_i']]:\n",
    "        \n",
    "        long_df = df[chunk].melt(id_vars='user_id', var_name='feature')  # Pivot to long format\n",
    "        long_df = long_df[long_df['value'] != 0]  # Remove nonzero elements\n",
    "        \n",
    "        yield long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id    feature  value\n",
      "0        x  feature_a      1\n",
      "1        y  feature_a      2\n",
      "2        z  feature_a      3\n",
      "4        x  feature_b      2\n",
      "5        y  feature_b      3\n",
      "8        x  feature_c      3\n",
      "10       z  feature_c      1\n",
      "\n",
      "   user_id    feature  value\n",
      "0        x  feature_d      6\n",
      "6        z  feature_e      1\n",
      "7        w  feature_e      3\n",
      "9        y  feature_f      4\n",
      "10       z  feature_f      2\n",
      "12       x  feature_g      7\n",
      "13       y  feature_g      8\n",
      "15       w  feature_g      1\n",
      "\n",
      "  user_id    feature  value\n",
      "5       y  feature_i      5\n",
      "6       z  feature_i      9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for long_df in get_df_chunks(all_df):\n",
    "    print(long_df, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we'll want to convert the `user_id` and `feature` columns to a categorical dtype and, to further reduce the memory footprint of our sparse matrix, we'll force the `value` to be `np.float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for long_df in get_df_chunks(all_df):\n",
    "    long_df[['user_id', 'feature']] = long_df[['user_id', 'feature']].astype('category')\n",
    "    long_df['value'] = long_df['value'].astype(np.float32)\n",
    "    \n",
    "    # Convert to sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Things Down\n",
    "\n",
    "### Keeping Track of the Data Chunks\n",
    "\n",
    "Since each `long_df` chunk may contain a different/overlapping set of `user_id` and `feature`, we'll need to keep track of them in the order in which they are observed so that we can cross reference our index-less sparse matrix with a nice `user_id` or `feature` lookup table. So, as each chunk is retrieved, we'll need to:\n",
    "\n",
    "1. Start with an empty `user_id` list of categories `all_user_ids`\n",
    "2. Get an ordered list of all of the `user_id` categories from the incoming `long_df`\n",
    "3. Append the incoming (ordered) `user_id` list to the (ordered) categories from the existing `all_user_ids` list and remove redundant categories\n",
    "4. Repeat steps 2-3 for all other chunks\n",
    "\n",
    "This is also done for `feature` as well. Now, you might be tempted use Python's built-in `set` operations to help identify a non-redundant list of `user_id` and `feature` but this is insufficient since order isn't maintained! So, instead, we initialize a couple of Pandas series for better bookkeeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = pd.Series([], dtype='category')\n",
    "all_features = pd.Series([], dtype='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll define a helper function to help us determine if we've already \"seen\" a certain category type before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_new_categories(add_cat, old_cat):\n",
    "    old_cat_set = set(old_cat)  # this reduces the lookup time from O(n) to O(1)\n",
    "    return [cat for cat in add_cat if cat not in old_cat_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as each chunk comes in, we'll update the observed `all_user_ids` and `all_features` categories on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = pd.Series([], dtype='category')\n",
    "all_features = pd.Series([], dtype='category')\n",
    "\n",
    "for i, long_df in enumerate(get_df_chunks(all_df)):\n",
    "    \n",
    "    long_df[['user_id', 'feature']] = long_df[['user_id', 'feature']].astype('category')\n",
    "    long_df['value'] = long_df['value'].astype(np.float32)\n",
    "\n",
    "    new_ids = return_new_categories(list(long_df['user_id'].cat.categories), list(all_user_ids.cat.categories))\n",
    "    all_user_ids.cat.add_categories(new_ids, inplace=True)\n",
    "    \n",
    "    new_features = return_new_categories(list(long_df['feature'].cat.categories), list(all_features.cat.categories))\n",
    "    all_features.cat.add_categories(new_features, inplace=True)\n",
    "    \n",
    "    long_df['user_id'].cat.set_categories(all_user_ids.cat.categories, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Sparse Array Using the PyData Sparse Package \n",
    "\n",
    "[Installing the PyData Sparse package](https://sparse.pydata.org/en/latest/install.html) should be as straigtforward as:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to their [documentation](https://sparse.pydata.org/en/latest/construct.html), you can construct a sparse nd-array (2D in our case) by doing:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> import sparse\n",
    "\n",
    ">>> coords = [[0, 1, 2, 3, 4],\n",
    "...           [0, 1, 2, 3, 4]]\n",
    ">>> data = [10, 20, 30, 40, 50]\n",
    ">>> s = sparse.COO(coords, data, shape=(5, 5))\n",
    "\n",
    ">>> s.todense()\n",
    "array([[10,  0,  0,  0,  0],\n",
    "       [ 0, 20,  0,  0,  0],\n",
    "       [ 0,  0, 30,  0,  0],\n",
    "       [ 0,  0,  0, 40,  0],\n",
    "       [ 0,  0,  0,  0, 50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we can do something similar with our incoming long dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = all_df.shape[0]  # This is the number of rows of user_id in the original dataframe\n",
    "ncol = long_df['feature'].cat.categories.shape[0]  # This is the number of unique features in this long_df\n",
    "\n",
    "coords = (long_df['user_id'].cat.codes.copy(), long_df['feature'].cat.codes.copy())\n",
    "data = long_df['value']\n",
    "\n",
    "some_sparse_array = sparse.COO(coords, data, shape=(nrow, ncol)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, voila, we have sparse 2D array! And as we process each incoming new `long_df`, we can add the new sparse array to our existing sparse array via the `concatenate` [function](https://sparse.pydata.org/en/latest/generated/sparse.concatenate.html#sparse.concatenate):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "existing_sparse_array = sparse.concatenate([existing_sparse_array, new_sparse_array], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting Things Altogether\n",
    "\n",
    "This is the workhorse so please review it carefully. A lot of this stuff might seem obvious or trivial but there are actually a lot of \"gotchas\" and one needs to take care and be mindful of the pitfalls of their approach. The biggest thing is making sure that the rows and columns our sparse array can be referenced correctly after we've stitched things back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = pd.Series([], dtype='category')\n",
    "all_features = pd.Series([], dtype='category')\n",
    "\n",
    "for i, long_df in enumerate(get_df_chunks(all_df)):\n",
    "    \n",
    "    long_df[['user_id', 'feature']] = long_df[['user_id', 'feature']].astype('category')\n",
    "    long_df['value'] = long_df['value'].astype(np.float32)\n",
    "\n",
    "    new_ids = return_new_categories(list(long_df['user_id'].cat.categories), list(all_user_ids.cat.categories))\n",
    "    all_user_ids.cat.add_categories(new_ids, inplace=True)\n",
    "    new_features = return_new_categories(list(long_df['feature'].cat.categories), list(all_features.cat.categories))\n",
    "    all_features.cat.add_categories(new_features, inplace=True)\n",
    "    \n",
    "    long_df['user_id'].cat.set_categories(all_user_ids.cat.categories, inplace=True)\n",
    "    \n",
    "    nrow = all_df.shape[0]\n",
    "    ncol = long_df['feature'].cat.categories.shape[0]\n",
    "    \n",
    "    coords = (long_df['user_id'].cat.codes.copy(), long_df['feature'].cat.codes.copy())\n",
    "    data = long_df['value']\n",
    "    \n",
    "    new_sparse_array = sparse.COO(coords, data, shape=(nrow, ncol)).astype(np.float32)\n",
    "    \n",
    "    if i == 0:\n",
    "        existing_sparse_array = new_sparse_array\n",
    "    else:\n",
    "        existing_sparse_array = sparse.concatenate([existing_sparse_array, new_sparse_array], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our final `existing_sparse_array` is sparse and has the correct dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<COO: shape=(4, 8), dtype=float32, nnz=17, fill_value=0.0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_sparse_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that our sparse array is the same as our original `all_df` dataframe, we can convert it back to dense form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 6., 0., 0., 7., 0.],\n",
       "       [2., 3., 0., 0., 0., 4., 8., 5.],\n",
       "       [3., 0., 1., 0., 1., 2., 0., 9.],\n",
       "       [0., 0., 0., 0., 3., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_sparse_array.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that it is nearly identical to `all_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>feature_a</th>\n",
       "      <th>feature_b</th>\n",
       "      <th>feature_c</th>\n",
       "      <th>feature_d</th>\n",
       "      <th>feature_e</th>\n",
       "      <th>feature_f</th>\n",
       "      <th>feature_g</th>\n",
       "      <th>feature_h</th>\n",
       "      <th>feature_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  feature_a  feature_b  feature_c  feature_d  feature_e  feature_f  \\\n",
       "0       x          1          2          3          6          0          0   \n",
       "1       y          2          3          0          0          0          4   \n",
       "2       z          3          0          1          0          1          2   \n",
       "3       w          0          0          0          0          3          0   \n",
       "\n",
       "   feature_g  feature_h  feature_i  \n",
       "0          7          0          0  \n",
       "1          8          0          5  \n",
       "2          0          0          9  \n",
       "3          1          0          0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observant individual will notice that `existing_sparse_array` has a missing column. That is, `feature_h`, which contains all zeros is completely missing and this is by design since that column contains no useful information from which to learn from. If you recall, this was handled earlier by our fake database generator, `get_df_chunks`, when we removed all nonzero elements before returning `long_df`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "long_df = long_df[long_df['value'] != 0]  # Remove nonzero elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check to make sure that our stored `all_features` categories did not accidentally capture `feature_h` and that the order of the categories should be retained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature_a', 'feature_b', 'feature_c', 'feature_d', 'feature_e',\n",
       "       'feature_f', 'feature_g', 'feature_i'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! You can also access the ordered list of `all_user_ids` via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x', 'y', 'z', 'w'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_user_ids.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or save the list to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids.cat.categories.to_series().to_csv(\"all_user_ids.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you wanted to, say, build an scikit-learn or XGBoost model then all you need to do is hand this sparse COO matrix over to the XGBoost model builder in `Compressed Sparse Row` format just as if it were a dense `numpy` array:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf.fit(existing_sparse_array.tocsr(), responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From our experience, and depending on your local hardware resources, we've been able to leverage this approach for data that contain around a million rows x a thousand features (or you can have many more rows if you decrease the number of features by an order of magnitude). Additionally, if your data is even bigger than this, then, depending on the type of model that you are trying to build, you may be able to use the scikit-learn's `partial_fit` (or sometimes called `warm_start`) function by breaking your data up into a smaller number of rows and feeding the chunks back in an iterative fashion.\n",
    "\n",
    "Hopefully, this approach will work for you as you venture down your data science journey! Let me know what you think in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
